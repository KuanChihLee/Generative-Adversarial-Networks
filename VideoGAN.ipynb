{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VideoGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KuanChihLee/Generative-Adversarial-Networks/blob/master/VideoGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bgp2FQPij2EH",
        "colab_type": "code",
        "outputId": "66d8d46b-696b-4a8e-e083-2e89307e88c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import multiply, Embedding, concatenate, Lambda\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, UpSampling2D, MaxPooling2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "from keras.backend import tf as ktf\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import os\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ntw47zE55_pL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONV_CHANNELS_G = [[128, 256, 128, 3],\n",
        "                   [128, 256, 128, 3],\n",
        "                   [128, 256, 512, 256, 128, 3],\n",
        "                   [128, 256, 512, 256, 128, 3]]\n",
        "\n",
        "CONV_KERNELS_G = [[3, 3, 3, 3],\n",
        "                  [5, 3, 3, 5],\n",
        "                  [5, 3, 3, 3, 3, 5],\n",
        "                  [7, 5, 5, 5, 5, 7]]\n",
        "\n",
        "CONV_CHANNELS_D = [[3, 64],\n",
        "                  [3, 64, 128, 128],\n",
        "                  [3, 128, 256, 256],\n",
        "                  [3, 128, 256, 512, 128]]\n",
        "\n",
        "CONV_KERNELS_D = [[3],\n",
        "                  [3, 3, 3],\n",
        "                  [5, 5, 5],\n",
        "                  [7, 7, 5, 5]]\n",
        "\n",
        "FC_LAYERS_D = [[512, 256, 1],\n",
        "              [1024, 512, 1],\n",
        "              [1024, 512, 1],\n",
        "              [1024, 512, 1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-qcxJtbpH4Z8",
        "colab_type": "code",
        "outputId": "8218ae56-5fd4-4b5c-bc1a-6ee040144eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Keq3VHNfIFmS",
        "colab_type": "code",
        "outputId": "9e945a57-aef5-45ea-d76d-059e2b602a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/GAN\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OUjUGvyeTdZ1",
        "colab_type": "code",
        "outputId": "d2189bf0-2902-4f19-ea36-98dc6cc86614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "def log10(t):\n",
        "    \"\"\"\n",
        "    Calculates the base-10 log of each element in t.\n",
        "    @param t: The tensor from which to calculate the base-10 log.\n",
        "    @return: A tensor with the base-10 log of each element in t.\n",
        "    \"\"\"\n",
        "    numerator = tf.log(t)\n",
        "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
        "    return numerator / denominator\n",
        "\n",
        "def get_dir(directory):\n",
        "    \"\"\"\n",
        "    Creates the given directory if it does not exist.\n",
        "    @param directory: The path to the directory.\n",
        "    @return: The path to the directory.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    return directory\n",
        "  \n",
        "def get_train_batch():\n",
        "    \"\"\"\n",
        "    Loads c.BATCH_SIZE clips from the database of preprocessed training clips.\n",
        "    @return: An array of shape\n",
        "            [c.BATCH_SIZE, c.TRAIN_HEIGHT, c.TRAIN_WIDTH, (3 * (c.HIST_LEN + 1))].\n",
        "    \"\"\"\n",
        "   \n",
        "    clips = np.empty([8, 32, 32, (3 * (4 + 1))],\n",
        "                     dtype=np.float32)\n",
        "    for i in range(8):\n",
        "        path = TRAIN_DIR_CLIPS + str(np.random.choice(NUM_CLIPS)) + '.npz'\n",
        "        clip = np.load(path)['arr_0']\n",
        "\n",
        "        clips[i] = clip\n",
        "\n",
        "    return clips\n",
        "  \n",
        "DATA_DIR = get_dir('./Data/')\n",
        "print(\"Data dir: \", DATA_DIR)\n",
        "TRAIN_DIR_CLIPS = get_dir(os.path.join(DATA_DIR, 'Clips/'))\n",
        "print(\"Train Data Clip dir: \", TRAIN_DIR_CLIPS)\n",
        "NUM_CLIPS = len(glob(TRAIN_DIR_CLIPS + '*'))\n",
        "print(\"Num of clips: \", NUM_CLIPS)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data dir:  ./Data/\n",
            "Train Data Clip dir:  ./Data/Clips/\n",
            "Num of clips:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aHUVYBhLeI-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VideoGAN:\n",
        "  def __init__(self):\n",
        "    \n",
        "    '''  Build network graph '''\n",
        "    print(\"Generator Initialization\")\n",
        "    self.G = self.generator(32, 32, 3, 210, 160, 3, CONV_CHANNELS_G, CONV_KERNELS_G)\n",
        "    \n",
        "    print(\"Discriminator Initialization\")\n",
        "    self.D = self.discriminator(32, 32, CONV_CHANNELS_D, CONV_KERNELS_D, FC_LAYERS_D)\n",
        "    \n",
        "    print('Optimizer Initialization')\n",
        "    optimizer = Adam(lr=0.0002, beta_1=0.5, decay=8e-8)\n",
        "    \n",
        "    self.D.compile(loss=self.custom_loss_functions_D(self.scale_preds_D), \n",
        "                   optimizer=optimizer, \n",
        "                   metrics=['accuracy'])\n",
        "    \n",
        "  def train(self, epochs):\n",
        "    \n",
        "    for cnt in range(epochs+1):\n",
        "      \n",
        "      print('Train discriminator...')\n",
        "      batch = get_train_batch()\n",
        "      input_frames = batch[:, :, :, :-3]\n",
        "      gt_frames = batch[:, :, :, -3:]\n",
        "      batch_size = np.shape(gt_frames)[0]\n",
        "      \n",
        "      #gen_img_frames = []\n",
        "      #gts_frames = []\n",
        "      #gen_output_frames = self.G.predict([input_frames, gt_frames])\n",
        "      #for i in range(self.scale_nets_num_G):\n",
        "      #    gen_img_frames.append(gen_output_frames[i])\n",
        "      #    gts_frames.append(gen_output_frames[i+self.scale_nets_num_G])\n",
        "      gen_img_frames = self.G.predict([input_frames, gt_frames])\n",
        "      #print(type(self.scale_preds_G[0]))\n",
        "      #print(type(self.scale_gts_G))\n",
        "      #print(type(gen_img_frames))\n",
        "\n",
        "      x_combined_batch = np.concatenate((gen_img_frames, gt_frames))\n",
        "      y_combined_batch = np.concatenate((np.zeros((batch_size, 1)), np.ones((batch_size, 1))))\n",
        "      d_loss = self.D.train_on_batch([x_combined_batch], [y_combined_batch]*self.scale_nets_num_D)\n",
        "      #print(type(self.scale_preds_D))\n",
        "      #print(type(self.scale_preds_D[0]))\n",
        "\n",
        "      print('Training generator...')\n",
        "      batch = get_train_batch()\n",
        "      input_frames = batch[:, :, :, :-3]\n",
        "      gt_frames = batch[:, :, :, -3:]\n",
        "      batch_size = np.shape(gt_frames)[0]\n",
        "      \n",
        "      gen_img_frames = self.G.predict([input_frames, gt_frames])\n",
        "      preds_D = self.D.predict([gen_img_frames])\n",
        "      \n",
        "      optimizer = Adam(lr=0.0002, beta_1=0.5, decay=8e-8)\n",
        "      self.G.compile(loss=self.custom_loss_functions_G(preds_D,self.scale_preds_G,self.scale_gts_G), \n",
        "                   optimizer=optimizer, \n",
        "                   metrics=['accuracy'])\n",
        "      g_loss = self.G.train_on_batch([input_frames, gt_frames], gt_frames)\n",
        "      print ('epoch: %d, [Discriminator loss: %f  Total loss: %f]' % (cnt, d_loss[0], g_loss[0])) \n",
        "      \n",
        " \n",
        "  def discriminator(self, height, width, \n",
        "                    conv_layers, conv_kernels, fc_layers):\n",
        "    \n",
        "    self.height_D = height\n",
        "    self.width_D = width\n",
        "   \n",
        "    self.conv_layers_D = conv_layers\n",
        "    self.conv_kernels_D = conv_kernels\n",
        "    self.fc_layers_D = fc_layers\n",
        "    self.scale_nets_num_D = len(conv_layers)\n",
        "    \n",
        "    self.scale_preds_D = []\n",
        "    \n",
        "    self.Input_frames = Input(shape=(self.height_D, self.width_D, self.conv_layers_D[0][0]))\n",
        "    \n",
        "    def __scaled_model_D():\n",
        "\n",
        "      New_scale_frames_shape = (self.scale_height_D, self.scale_width_D, self.conv_layers_D[self.net_num_D][0])\n",
        "      New_scale_frames = Input(shape=New_scale_frames_shape)\n",
        "      \n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(self.conv_layers_D[self.net_num_D][1], self.conv_kernels_D[self.net_num_D][0], padding='same', input_shape=New_scale_frames_shape))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "      \n",
        "      for layer in range(2,len(self.conv_layers_D[self.net_num_D])):\n",
        "        model.add(Conv2D(self.conv_layers_D[self.net_num_D][layer], self.conv_kernels_D[self.net_num_D][layer-1], padding='same', activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
        "        model.add(Dropout(0.2))\n",
        "      \n",
        "      model.add(Flatten())\n",
        "      for fc_layer in range(len(self.fc_layers_D[self.net_num_D])):\n",
        "        if fc_layer == len(self.fc_layers_D[self.net_num_D]) - 1:\n",
        "          model.add(Dense(self.fc_layers_D[self.net_num_D][fc_layer]))\n",
        "          model.add(Activation('sigmoid'))\n",
        "          current_scale_pred_D = model(New_scale_frames)\n",
        "          \n",
        "          return Model([New_scale_frames], current_scale_pred_D)\n",
        "        else:\n",
        "          model.add(Dense(self.fc_layers_D[self.net_num_D][fc_layer]))\n",
        "          model.add(BatchNormalization(momentum=0.8))\n",
        "          model.add(Activation('relu'))\n",
        "          model.add(Dropout(0.2))\n",
        "    \n",
        "    for net_num in range(self.scale_nets_num_D):  \n",
        "      \n",
        "      self.net_num_D = net_num      \n",
        "      scale_factor = 1. / 2 ** ((self.scale_nets_num_D - 1) - self.net_num_D)\n",
        "      self.scale_height_D = int(self.height_D * scale_factor)\n",
        "      self.scale_width_D = int(self.width_D * scale_factor)\n",
        "         \n",
        "      scale_inputs_frames = Lambda(lambda image: ktf.image.resize_images(image, (self.scale_height_D, self.scale_width_D)))(self.Input_frames)\n",
        "      \n",
        "      scale_model = __scaled_model_D()\n",
        "      current_scale_pred = scale_model([scale_inputs_frames])\n",
        "      self.scale_preds_D.append(current_scale_pred)\n",
        "      \n",
        "    return Model([self.Input_frames], self.scale_preds_D)\n",
        "  \n",
        "  \n",
        "  def generator(self, height_train, width_train, channel_train, \n",
        "                      height_ground, width_ground, channel_ground, \n",
        "                      conv_layers, conv_kernels):\n",
        "\n",
        "    self.height_G = height_train\n",
        "    self.width_G = width_train\n",
        "    self.channel_G = channel_train\n",
        "    self.conv_layers_G = conv_layers\n",
        "    self.conv_kernels_G = conv_kernels\n",
        "    self.scale_nets_num_G = len(conv_layers)\n",
        "    \n",
        "    self.scale_preds_G = []  \n",
        "    self.scale_gts_G = []  \n",
        "    self.__scale_preds_G = []  \n",
        "\n",
        "    Input_frames = Input(shape=(self.height_G, self.width_G, self.channel_G * 4))\n",
        "    Input_gt_frames = Input(shape=(self.height_G, self.width_G, self.channel_G))\n",
        "     \n",
        "    def __scaled_model_G(model, combined_frames, input_frames, last_scale_frames):\n",
        "      \n",
        "      for layer in range(1,len(self.conv_layers_G[self.net_num])):\n",
        "        if layer == len(self.conv_layers_G[self.net_num]) - 1:\n",
        "          if self.net_num != self.scale_nets_num_G - 1:\n",
        "            model.add(UpSampling2D())\n",
        "          model.add(Conv2D(self.conv_layers_G[self.net_num][layer], self.conv_kernels_G[self.net_num][layer], padding='same'))\n",
        "          model.add(Activation('tanh'))\n",
        "          current_scale_pred_train = model(combined_frames)\n",
        "          #print(\"Input Shape: \", input_frames.shape)\n",
        "          #print(\"Output Shape: \", current_scale_pred_train.shape)\n",
        "          \n",
        "          return Model([input_frames, last_scale_frames], current_scale_pred_train)\n",
        "        else:\n",
        "          model.add(Conv2D(self.conv_layers_G[self.net_num][layer], self.conv_kernels_G[self.net_num][layer], padding='same'))\n",
        "          model.add(BatchNormalization(momentum=0.8))\n",
        "          model.add(Activation('relu'))\n",
        "          model.add(Dropout(0.2))\n",
        "    \n",
        "    def __scaled_model_G_1():\n",
        "\n",
        "      New_scale_frames = Input(shape=(self.scale_height_G, self.scale_width_G, self.channel_G * 4))\n",
        "      Last_scale_frames = Input(shape=(self.scale_height_G, self.scale_width_G, self.channel_G))\n",
        "      New_scale_frames_shape = (self.scale_height_G, self.scale_width_G, self.channel_G * 4)\n",
        "      \n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(self.conv_layers_G[self.net_num][0], self.conv_kernels_G[self.net_num][0], padding='same', input_shape=New_scale_frames_shape))\n",
        "      model.add(BatchNormalization(momentum=0.8))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "      \n",
        "      return __scaled_model_G(model, New_scale_frames, New_scale_frames, Last_scale_frames)\n",
        "    \n",
        "    def __scaled_model_G_hiddens():\n",
        "\n",
        "      New_scale_frames = Input(shape=(self.scale_height_G, self.scale_width_G, self.channel_G * 4))\n",
        "      Last_scale_frames = Input(shape=(self.scale_height_G, self.scale_width_G, self.channel_G))\n",
        "      New_combined_frames = concatenate([New_scale_frames, Last_scale_frames], axis=-1)\n",
        "      \n",
        "      New_combined_frames_shape = (self.scale_height_G, self.scale_width_G, self.channel_G * 5)\n",
        "      \n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(self.conv_layers_G[self.net_num][0], self.conv_kernels_G[self.net_num][0], padding='same', input_shape=New_combined_frames_shape))\n",
        "      model.add(BatchNormalization(momentum=0.8))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "      \n",
        "      return __scaled_model_G(model, New_combined_frames, New_scale_frames, Last_scale_frames)\n",
        "   \n",
        "    def __resize_img(image):\n",
        "      resized = ktf.image.resize_images(image, (self.scale_height_G, self.scale_width_G))\n",
        "      return resized\n",
        "  \n",
        "    self.net_num = 0\n",
        "    scale_factor = 1. / 2 ** ((self.scale_nets_num_G - 1) - self.net_num)\n",
        "    self.scale_height_G = int(self.height_G * scale_factor)\n",
        "    self.scale_width_G = int(self.width_G * scale_factor)\n",
        "\n",
        "    scale_inputs_frames = Lambda(__resize_img, input_shape=(self.height_G, self.width_G, self.channel_G * 4), \n",
        "                                 output_shape=(self.scale_height_G, self.scale_width_G, self.channel_G * 4))(Input_frames)\n",
        "    scale_gts_frames = Lambda(__resize_img, input_shape=(self.height_G, self.width_G, self.channel_G), \n",
        "                              output_shape=(self.scale_height_G, self.scale_width_G, self.channel_G))(Input_gt_frames)\n",
        "\n",
        "    hidden_model = __scaled_model_G_1()\n",
        "    self.__scale_preds_G.append(hidden_model([scale_inputs_frames,scale_gts_frames]))\n",
        "\n",
        "    self.scale_gts_G.append(scale_gts_frames)\n",
        "    self.scale_preds_G.append(Lambda(__resize_img, output_shape=(self.scale_height_G, self.scale_width_G, self.channel_G))(self.__scale_preds_G[self.net_num]))\n",
        "\n",
        "    for net_num in range(1,self.scale_nets_num_G):  \n",
        "      \n",
        "      self.net_num = net_num  \n",
        "      scale_factor = 1. / 2 ** ((self.scale_nets_num_G - 1) - self.net_num)\n",
        "      self.scale_height_G = int(self.height_G * scale_factor)\n",
        "      self.scale_width_G = int(self.width_G * scale_factor)\n",
        "\n",
        "      scale_inputs_frames = Lambda(__resize_img, input_shape=(self.height_G, self.width_G, self.channel_G * 4), \n",
        "                                 output_shape=(self.scale_height_G, self.scale_width_G, self.channel_G * 4))(Input_frames)\n",
        "      scale_gts_frames = Lambda(__resize_img, input_shape=(self.height_G, self.width_G, self.channel_G), \n",
        "                              output_shape=(self.scale_height_G, self.scale_width_G, self.channel_G))(Input_gt_frames)\n",
        "            \n",
        "      last_scale_pred = self.__scale_preds_G[self.net_num - 1]\n",
        "\n",
        "      hidden_model = __scaled_model_G_hiddens()\n",
        "      self.__scale_preds_G.append(hidden_model([scale_inputs_frames,last_scale_pred]))\n",
        "      \n",
        "      self.scale_gts_G.append(scale_gts_frames)\n",
        "      self.scale_preds_G.append(Lambda(__resize_img, output_shape=(self.scale_height_G, self.scale_width_G, self.channel_G))(self.__scale_preds_G[self.net_num]))\n",
        "    \n",
        "    #for gts in self.scale_gts_G:\n",
        "    #  self.scale_preds_G.append(gts)\n",
        "      \n",
        "    return Model([Input_frames, Input_gt_frames], self.scale_preds_G[-1])\n",
        "    \n",
        "  \n",
        "  def custom_loss_functions_D(self,preds):\n",
        "    \n",
        "    def adv_loss(y_true,_):\n",
        "      scale_losses = []\n",
        "      for i in range(len(preds)):\n",
        "          loss = bce_loss(preds[i], y_true)\n",
        "          scale_losses.append(loss)\n",
        "      return tf.reduce_mean(tf.stack(scale_losses))\n",
        "    def bce_loss(preds, targets):\n",
        "      return tf.squeeze(-1 * (tf.matmul(targets, log10(preds), transpose_a=True) +\n",
        "                              tf.matmul(1 - targets, log10(1 - preds), transpose_a=True)))\n",
        "    return adv_loss \n",
        "  \n",
        "  \n",
        "  def custom_loss_functions_G(self,preds_D,gen_img_G,gts_G,\n",
        "                              lam_adv=1,lam_lp=1,lam_gdl=1,l_num=2,alpha=2):\n",
        "    def combined_loss(y_true,_):\n",
        "      batch_size = tf.shape(gen_img_G[0])[0]\n",
        "      loss = lam_lp * lp_loss(gen_img_G, gts_G, l_num)\n",
        "      loss += lam_gdl * gdl_loss(gen_img_G, gts_G, alpha)  \n",
        "      #loss += lam_adv * adv_loss(preds_D, tf.ones([batch_size, 1]))\n",
        "      return loss   \n",
        "              \n",
        "    def adv_loss(preds, labels):\n",
        "      scale_losses = []\n",
        "      for i in range(len(preds)):\n",
        "          loss = bce_loss(preds[i], labels)\n",
        "          scale_losses.append(loss)\n",
        "      return tf.reduce_mean(tf.stack(scale_losses))\n",
        "    \n",
        "    def bce_loss(preds, targets):\n",
        "      return tf.squeeze(-1 * (tf.matmul(targets, log10(preds), transpose_a=True) +\n",
        "                              tf.matmul(1 - targets, log10(1 - preds), transpose_a=True)))\n",
        "    \n",
        "    def lp_loss(gen_frames, gt_frames, l_num):\n",
        "      scale_losses = []\n",
        "      for i in range(len(gen_frames)):\n",
        "        scale_losses.append(tf.reduce_sum(tf.abs(gen_frames[i] - gt_frames[i])**l_num))\n",
        "      return tf.reduce_mean(tf.stack(scale_losses))\n",
        "    \n",
        "    def gdl_loss(gen_frames, gt_frames, alpha):\n",
        "      scale_losses = []\n",
        "      for i in range(len(gen_frames)):\n",
        "        # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
        "        pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
        "        neg = -1 * pos\n",
        "        filter_x = tf.expand_dims(tf.stack([neg, pos]), 0)  # [-1, 1]\n",
        "        filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
        "        strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
        "        padding = 'SAME'\n",
        "\n",
        "        gen_dx = tf.abs(tf.nn.conv2d(gen_frames[i], filter_x, strides, padding=padding))\n",
        "        gen_dy = tf.abs(tf.nn.conv2d(gen_frames[i], filter_y, strides, padding=padding))\n",
        "        gt_dx = tf.abs(tf.nn.conv2d(gt_frames[i], filter_x, strides, padding=padding))\n",
        "        gt_dy = tf.abs(tf.nn.conv2d(gt_frames[i], filter_y, strides, padding=padding))\n",
        "\n",
        "        grad_diff_x = tf.abs(gt_dx - gen_dx)\n",
        "        grad_diff_y = tf.abs(gt_dy - gen_dy)\n",
        "\n",
        "        scale_losses.append(tf.reduce_sum((grad_diff_x ** alpha + grad_diff_y ** alpha)))\n",
        "      return tf.reduce_mean(tf.stack(scale_losses))\n",
        "    \n",
        "    return combined_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8azKPQLDAg6",
        "colab_type": "code",
        "outputId": "a82eaba0-7280-4a96-cbe5-e3aa79596ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2498
        }
      },
      "cell_type": "code",
      "source": [
        "model = VideoGAN()\n",
        "model.train(24)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator Initialization\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Discriminator Initialization\n",
            "Optimizer Initialization\n",
            "Train discriminator...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Training generator...\n",
            "epoch: 0, [Discriminator loss: 20.228703  Total loss: 17211.798828]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 1, [Discriminator loss: 42.028423  Total loss: 11223.508789]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 2, [Discriminator loss: 35.112808  Total loss: 8658.461914]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 3, [Discriminator loss: 11.935239  Total loss: 7845.647949]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 4, [Discriminator loss: 10.460975  Total loss: 6838.465820]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 5, [Discriminator loss: 7.138430  Total loss: 7449.572754]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 6, [Discriminator loss: 5.895762  Total loss: 6033.153320]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 7, [Discriminator loss: 4.819715  Total loss: 5061.006836]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 8, [Discriminator loss: 13.276580  Total loss: 5070.564453]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 9, [Discriminator loss: 3.928351  Total loss: 4472.983398]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 10, [Discriminator loss: 3.298314  Total loss: 4333.039551]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 11, [Discriminator loss: 13.357904  Total loss: 4154.940918]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 12, [Discriminator loss: 15.992505  Total loss: 3673.973389]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 13, [Discriminator loss: 7.584388  Total loss: 3716.602295]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 14, [Discriminator loss: 7.962851  Total loss: 3950.280273]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 15, [Discriminator loss: 7.766358  Total loss: 3668.440674]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 16, [Discriminator loss: 3.080464  Total loss: 2933.258545]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 17, [Discriminator loss: 5.028281  Total loss: 3032.387207]\n",
            "Train discriminator...\n",
            "Training generator...\n",
            "epoch: 18, [Discriminator loss: 7.388085  Total loss: 3042.685791]\n",
            "Train discriminator...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b4e1bf2d03de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-9f1222a0b42d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m#    gen_img_frames.append(gen_output_frames[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m#    gts_frames.append(gen_output_frames[i+self.scale_nets_num_G])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mgen_img_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_frames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0;31m#print(type(self.scale_preds_G[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m#print(type(self.scale_gts_G))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dcKXbvn4JJ6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}